{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c69e1c",
   "metadata": {},
   "source": [
    "#### SimpleTransformers\n",
    "Few resources :\n",
    "1. https://www.philschmid.de/bert-text-classification-in-a-different-language\n",
    "2. https://github.com/ThilinaRajapakse/simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe94a6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simpletransformers\n",
      "  Downloading simpletransformers-0.63.9-py3-none-any.whl (250 kB)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from simpletransformers) (4.64.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from simpletransformers) (1.4.4)\n",
      "Collecting transformers>=4.6.0\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from simpletransformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from simpletransformers) (2.27.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from simpletransformers) (1.0.2)\n",
      "Requirement already satisfied: tensorboard in c:\\programdata\\anaconda3\\lib\\site-packages (from simpletransformers) (2.11.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from simpletransformers) (1.21.5)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Collecting wandb>=0.10.32\n",
      "  Downloading wandb-0.13.10-py3-none-any.whl (2.0 MB)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from simpletransformers) (1.7.3)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.18.1-py2.py3-none-any.whl (9.6 MB)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.47.0->simpletransformers) (0.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers>=4.6.0->simpletransformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers>=4.6.0->simpletransformers) (3.0.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (5.8.0)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.15.0-py2.py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (3.19.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (61.2.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (3.1.27)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-win_amd64.whl (11 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (8.0.4)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb>=0.10.32->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (1.26.9)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (9.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (2022.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "layer 0.10.3081287781 requires aiohttp<3.8.0,>=3.7.3, but you have aiohttp 3.8.3 which is incompatible.\n",
      "layer 0.10.3081287781 requires pandas==1.3.5, but you have pandas 1.4.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dill<0.3.7\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (3.8.3)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->simpletransformers) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->simpletransformers) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->simpletransformers) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->simpletransformers) (1.1.0)\n",
      "Collecting tzlocal>=1.1\n",
      "  Downloading tzlocal-4.2-py3-none-any.whl (19 kB)\n",
      "Collecting validators>=0.2\n",
      "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
      "Requirement already satisfied: cachetools>=4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (4.2.2)\n",
      "Requirement already satisfied: watchdog in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (2.1.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (9.0.1)\n",
      "Collecting altair>=3.2.0\n",
      "  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "Collecting blinker>=1.0.0\n",
      "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (4.11.3)\n",
      "Requirement already satisfied: toml in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (0.10.2)\n",
      "Collecting pydeck>=0.1.dev5\n",
      "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
      "Requirement already satisfied: tornado>=6.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (6.1)\n",
      "Collecting semver\n",
      "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (12.5.1)\n",
      "Collecting pympler>=0.9\n",
      "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (4.4.0)\n",
      "Requirement already satisfied: toolz in c:\\programdata\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.11.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.3)\n",
      "Requirement already satisfied: entrypoints in c:\\programdata\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.4->streamlit->simpletransformers) (3.7.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (2.0.1)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->streamlit->simpletransformers) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.11.2)\n",
      "Collecting pytz-deprecation-shim\n",
      "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tzdata\n",
      "  Downloading tzdata-2022.7-py2.py3-none-any.whl (340 kB)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from validators>=0.2->streamlit->simpletransformers) (5.1.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (1.33.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (2.0.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (1.3.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (1.49.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (0.4.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.2.1)\n",
      "Building wheels for collected packages: pathtools, seqeval, validators\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=622bb6fa2cd58ef146d2bdcba62c733266afed713034a58e7c64aa23e8d073fe\n",
      "  Stored in directory: c:\\users\\chopr.000\\appdata\\local\\pip\\cache\\wheels\\b7\\0a\\67\\ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=4ba0a460efb5346522427837914151a0c50a07519e98fba268a00222620620ed\n",
      "  Stored in directory: c:\\users\\chopr.000\\appdata\\local\\pip\\cache\\wheels\\e2\\a5\\92\\2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
      "  Building wheel for validators (setup.py): started\n",
      "  Building wheel for validators (setup.py): finished with status 'done'\n",
      "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=bf2fb61e9b4f40845c233744b12c19a45d6efecf40b12d9aaae5c0d6d3ff9b31\n",
      "  Stored in directory: c:\\users\\chopr.000\\appdata\\local\\pip\\cache\\wheels\\2d\\f0\\a8\\1094fca7a7e5d0d12ff56e0c64675d72aa5cc81a5fc200e849\n",
      "Successfully built pathtools seqeval validators\n",
      "Installing collected packages: urllib3, tzdata, pytz-deprecation-shim, dill, xxhash, validators, tzlocal, tokenizers, setproctitle, sentry-sdk, semver, responses, pympler, pydeck, pathtools, multiprocess, huggingface-hub, docker-pycreds, blinker, altair, wandb, transformers, streamlit, seqeval, sentencepiece, datasets, simpletransformers\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.9\n",
      "    Uninstalling urllib3-1.26.9:\n",
      "      Successfully uninstalled urllib3-1.26.9\n",
      "Successfully installed altair-4.2.2 blinker-1.5 datasets-2.9.0 dill-0.3.6 docker-pycreds-0.4.0 huggingface-hub-0.12.1 multiprocess-0.70.14 pathtools-0.1.2 pydeck-0.8.0 pympler-1.0.1 pytz-deprecation-shim-0.1.0.post0 responses-0.18.0 semver-2.13.0 sentencepiece-0.1.97 sentry-sdk-1.15.0 seqeval-1.2.2 setproctitle-1.3.2 simpletransformers-0.63.9 streamlit-1.18.1 tokenizers-0.13.2 transformers-4.26.1 tzdata-2022.7 tzlocal-4.2 urllib3-1.26.14 validators-0.20.0 wandb-0.13.10 xxhash-3.2.0\n"
     ]
    }
   ],
   "source": [
    "#Uncomment if not installed\n",
    "#pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ac5a246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 15  531k   15 81920    0     0  49825      0  0:00:10  0:00:01  0:00:09 49829\n",
      "100  531k  100  531k    0     0   236k      0  0:00:02  0:00:02 --:--:--  236k\n"
     ]
    }
   ],
   "source": [
    "#curl https://projects.fzai.h-da.de/iggsa/wp-content/uploads/2019/08/germeval2019GoldLabelsSubtask1_2.txt\n",
    "#curl https://projects.fzai.h-da.de/iggsa/wp-content/uploads/2019/09/germeval2019.training_subtask1_2_korrigiert.txt\n",
    "\n",
    "##The above two downloads have been moved to the following two places\n",
    "\n",
    "#!curl https://fz.h-da.de/iggsa//wp-content/uploads/2019/08/germeval2019GoldLabelsSubtask1_2.txt --output germeval2019.GoldLabelsSubtask1_2.txt\n",
    "#!curl https://fz.h-da.de/iggsa//wp-content/uploads/2019/09/germeval2019.training_subtask1_2_korrigiert.txt --output germeval2019.training_subtask1_2_korrigiert.txt\n",
    "    \n",
    "!curl https://fz.h-da.de/fileadmin/user_upload/germeval2019GoldLabelsSubtask1_2.txt --output germeval2019GoldLabelsSubtask1_2.txt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a92dd24",
   "metadata": {},
   "source": [
    "##### Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7da8b5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>pred_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@JanZimmHHB @mopo Komisch das die Realitätsver...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@faznet @Gruene_Europa @SPDEuropa @CDU CDU ste...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DLFNachrichten Die Gesichter, Namen, Religion...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@welt Wie verwirrt muss man sein um sich zu we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@hacker_1991 @torben_braga Weil die AfD den Fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  pred_class\n",
       "0  @JanZimmHHB @mopo Komisch das die Realitätsver...           3\n",
       "1  @faznet @Gruene_Europa @SPDEuropa @CDU CDU ste...           1\n",
       "2  @DLFNachrichten Die Gesichter, Namen, Religion...           0\n",
       "3  @welt Wie verwirrt muss man sein um sich zu we...           1\n",
       "4  @hacker_1991 @torben_braga Weil die AfD den Fe...           1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df1=pd.read_csv('germeval2019GoldLabelsSubtask1_2.txt',sep='\\t', lineterminator='\\n',encoding='utf8',names=[\"tweet\", \"task1\", \"task2\"])\n",
    "\n",
    "'''\n",
    "# to clear trailing '\\r' from task2 column\n",
    "for i,elem in enumerate(df1['task2']):\n",
    "    df1['task2'][i]=df1['task2'][i].replace('\\r','')\n",
    "'''\n",
    "\n",
    "df1['task2']=df1['task2'].apply(lambda x: x.replace('\\r',''))\n",
    "\n",
    "#class_list=list(set(df1['task2']))\n",
    "\n",
    "\n",
    "class_list=['OTHER', 'ABUSE', 'PROFANITY', 'INSULT']\n",
    "\n",
    "\n",
    "df1['pred_class']=df1['task2'].apply(lambda x:class_list.index(x))\n",
    "\n",
    "df = df1[['tweet','pred_class']]\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "65b85dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OTHER', 'ABUSE', 'PROFANITY', 'INSULT']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2105a9",
   "metadata": {},
   "source": [
    "##### Train Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2824364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train,df_test=train_test_split(df,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "776a61da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e643ec123eca48689f51350a38a25032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/270M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-german-cased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-german-cased and are newly initialized: ['encoder.layer.3.attention.self.value.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'classifier.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'pooler.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'classifier.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.2.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.attention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3e4d38c6f747d0be6229ca7934ec44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3c0e8e307a45c4a71fa75cad3a2069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/240k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e7d78d1941420a8a2c2b3da0196c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/479k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "# define hyperparameter\n",
    "train_args ={\n",
    "            \"reprocess_input_data\": True,\n",
    "            \"fp16\":False,\n",
    "            #\"num_train_epochs\": 4\n",
    "            \"num_train_epochs\": 1\n",
    "            }\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(\n",
    "                            \"bert\", \"distilbert-base-german-cased\",\n",
    "                            num_labels=4,\n",
    "                            use_cuda=False,\n",
    "                            args=train_args\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91123d45",
   "metadata": {},
   "source": [
    "##### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7e3bfeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d462d14bb1045b7b4d3f52891707a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2727 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345bdc2a6a90435794389e19bb588306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a917d3e110b54dafa082874bc1254f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 4:   0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd6b0eb8ea6421885b5495e7e5d0dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 4:   0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a84ed9588b24281a32655a527e1ec40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 4:   0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ccc2b16671b496a99812d4664137392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 4:   0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1364, 0.8648303075313918)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_model(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f3f9a",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d35cf502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf23bd8314a483f9106a9fe82487146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0401b775a649eca8f9648c7aa29d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average='micro')\n",
    "\n",
    "result, model_outputs, wrong_predictions = model.eval_model(df_test, f1=f1_multiclass, acc=accuracy_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c4a83860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "def pack_model(model_path='',file_name=''):\n",
    "  files = [files for root, dirs, files in os.walk(model_path)][0]\n",
    "  with tarfile.open(file_name+ '.tar.gz', 'w:gz') as f:\n",
    "    for file in files:\n",
    "      f.add(f'{model_path}/{file}')\n",
    "\n",
    "# run the function\n",
    "pack_model('outputs','germeval-distilbert-german')\n",
    "#save_model('outputs','germeval-distilbert-german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f3f8a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_model(model_name=''):\n",
    "  tar = tarfile.open(f\"{model_name}.tar.gz\", \"r:gz\")\n",
    "  tar.extractall()\n",
    "  tar.close()\n",
    "\n",
    "unpack_model('germeval-distilbert-german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ffa9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
